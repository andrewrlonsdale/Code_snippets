!pip install mdbtools

import subprocess
import pandas as pd

filename = 'your_mdb_file.mdb'

# Get a list of table names in the MDB file
tables = subprocess.check_output(['mdb-tables', '-1', filename]).decode().split()

# Loop over the table names and export each table to a CSV file
for table in tables:
    subprocess.call(['mdb-export', '-I', 'csv', '-q', '"', filename, table], stdout=open(f'{table}.csv', 'w'))

    # Load the CSV file into a Pandas dataframe
    df = pd.read_csv(f'{table}.csv')

    # Write the dataframe to Parquet using AWS Glue Wrangler
    import awswrangler as wr

    wr.s3.to_parquet(
        df=df,
        path=f's3://your-bucket/{table}.parquet',
        dataset=True,
        database='your_database',
        table=table,
        mode='overwrite'
    )



import pyodbc
import pandas as pd

# Set up a connection to the MDB file using pyodbc
conn_str = r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=path/to/your_mdb_file.mdb;'
conn = pyodbc.connect(conn_str)

# Get a list of table names in the MDB file
tables = [table.table_name for table in conn.cursor().tables()]

# Loop over the table names and load each table into a Pandas dataframe
for table in tables:
    df = pd.read_sql(f'SELECT * FROM {table}', conn)

    # Write the dataframe to Parquet using AWS Glue Wrangler
    import awswrangler as wr

    wr.s3.to_parquet(
        df=df,
        path=f's3://your-bucket/{table}.parquet',
        dataset=True,
        database='your_database',
        table=table,
        mode='overwrite'
    )



import mdb_py
import pandas as pd
import awswrangler as wr

# Open the MDB file using mdb_py
mdb_file = mdb_py.load('path/to/your_mdb_file.mdb')

# Get a list of table names in the MDB file
tables = mdb_file.tables()

# Loop over the table names and load each table into a Pandas dataframe
for table in tables:
    # Load the table data into a list of dicts
    data = mdb_file[table].all()

    # Convert the list of dicts to a Pandas dataframe
    df = pd.DataFrame(data)

    # Write the dataframe to Parquet using AWS Glue Wrangler
    wr.s3.to_parquet(
        df=df,
        path=f's3://your-bucket/{table}.parquet',
        dataset=True,
        database='your_database',
        table=table,
        mode='overwrite'
    )
