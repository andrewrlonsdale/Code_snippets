from pyathena import connect

# Connect to AWS Athena
conn = connect(aws_access_key_id='your_access_key_id',
                aws_secret_access_key='your_secret_access_key',
                s3_staging_dir='s3://your_bucket/path/to/staging/dir',
                region_name='your_region')

# Define the name of the table to count rows in
table_name = "sample_table"

# Execute a query to count the number of rows in the table
cursor = conn.cursor()
cursor.execute(f"SELECT COUNT(*) FROM {table_name}")

# Get the result of the query
result = cursor.fetchone()

# Print the number of rows
print(result[0])


import pyathena

conn = pyathena.connect(aws_access_key_id='ACCESS_KEY',
                        aws_secret_access_key='SECRET_KEY',
                        s3_staging_dir='s3://YOUR_BUCKET/path',
                        region_name='REGION')

cursor = conn.cursor()

import pandas as pd

df = pd.DataFrame({'col1': [1, 2], 'col2': ['a', 'b']})

df.to_csv('s3://YOUR_BUCKET/path/data.csv', index=False)


table_name = 'YOUR_TABLE_NAME'

create_table_query = f'''
CREATE EXTERNAL TABLE IF NOT EXISTS {table_name} (
    col1 INT,
    col2 STRING
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION 's3://YOUR_BUCKET/path/'
TBLPROPERTIES ('skip.header.line.count'='1')
'''

cursor.execute(create_table_query)


select_query = f'SELECT * FROM {table_name}'
cursor.execute(select_query)
print(cursor.fetchall())
