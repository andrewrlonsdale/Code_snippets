import json
import re
from collections import defaultdict

def extract_code_from_notebook(notebook_path):
    """Extracts code cells from a Jupyter notebook"""
    with open(notebook_path, "r", encoding="utf-8") as f:
        notebook = json.load(f)
    
    code_cells = [
        cell["source"] for cell in notebook.get("cells", []) if cell["cell_type"] == "code"
    ]
    return "\n".join(["".join(cell) for cell in code_cells])

def parse_assignments(source_code):
    """
    Parses variable assignments in the given code.
    
    Returns:
    - assignment_map: { var_name -> [list of its assignments] }
    - dependencies: { var_name -> [list of vars used to create it] }
    """
    assignment_map = defaultdict(list)
    dependencies = defaultdict(list)

    assignment_pattern = re.compile(r"(\w+)\s*=\s*(.+)")
    merge_pattern = re.compile(r"(\w+)\.merge\((\w+)")
    sql_pattern = re.compile(r"(?:pd|spark)\.read_sql\(['\"](.+?)['\"]")
    dataframe_read_pattern = re.compile(r"(?:pd|spark)\.read_(\w+)\((.+?)\)")

    lines = source_code.split("\n")

    for line in lines:
        match = assignment_pattern.match(line)
        if match:
            var_name, expression = match.groups()
            assignment_map[var_name].append(expression.strip())

            # Track dependencies
            var_refs = re.findall(r"(\b\w+\b)", expression)  # Find variable-like references
            dependencies[var_name].extend(var for var in var_refs if var in assignment_map)

            # Check for merge operations
            merge_match = merge_pattern.search(expression)
            if merge_match:
                left_df, right_df = merge_match.groups()
                dependencies[var_name].extend([left_df, right_df])

            # Detect SQL queries
            sql_match = sql_pattern.search(expression)
            if sql_match:
                assignment_map[var_name].append(f"SQL Query: {sql_match.group(1)}")

            # Detect DataFrame file reads
            df_read_match = dataframe_read_pattern.search(expression)
            if df_read_match:
                read_type, source = df_read_match.groups()
                assignment_map[var_name].append(f"File Read ({read_type}): {source}")

    return assignment_map, dependencies

def trace_variable(assignment_map, dependencies, target_var):
    """
    Traces the origin of a target variable, considering complex operations.
    
    Returns:
    - List of all detected sources (SQL tables, files, or original DataFrames)
    """
    traced_sources = []
    seen_vars = set()
    stack = [target_var]

    while stack:
        current_var = stack.pop()
        if current_var in seen_vars:
            continue  # Prevent infinite loops
        seen_vars.add(current_var)

        if current_var in assignment_map:
            last_assignment = assignment_map[current_var][-1]

            # If SQL or file read, it's a source
            if "SQL Query:" in last_assignment or "File Read" in last_assignment:
                traced_sources.append(f"{current_var}: {last_assignment}")

            # Add dependencies (e.g., in case of merges or chained transformations)
            if current_var in dependencies:
                stack.extend(dependencies[current_var])

    return traced_sources if traced_sources else [f"No clear source found for {target_var}"]

# Example Usage
notebook_path = "your_notebook.ipynb"  # Replace with the actual notebook path
variable_to_trace = "df_2"  # Replace with your target variable

source_code = extract_code_from_notebook(notebook_path)
assignment_map, dependencies = parse_assignments(source_code)
origin_sources = trace_variable(assignment_map, dependencies, variable_to_trace)

print("Tracing results:")
for source in origin_sources:
    print(source)
