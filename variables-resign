import json
import re
from collections import defaultdict

def extract_code_from_notebook(notebook_path):
    """Extracts code cells from a Jupyter notebook"""
    with open(notebook_path, "r", encoding="utf-8") as f:
        notebook = json.load(f)
    
    code_cells = [
        cell["source"] for cell in notebook.get("cells", []) if cell["cell_type"] == "code"
    ]
    return "\n".join(["".join(cell) for cell in code_cells])

def parse_assignments(source_code):
    """
    Parses variable assignments and dependencies in the given code.
    
    Returns:
    - assignment_map: { var_name -> [list of its assignments] }
    - dependencies: { var_name -> [list of vars used to create it] }
    """
    assignment_map = defaultdict(list)
    dependencies = defaultdict(list)

    assignment_pattern = re.compile(r"(\w+)\s*=\s*(.+)")
    merge_pattern = re.compile(r"(\w+)\.merge\((\w+)")
    sql_pattern = re.compile(r"(?:pd|spark)\.read_sql\(['\"](.+?)['\"]")
    dataframe_read_pattern = re.compile(r"(?:pd|spark)\.read_(\w+)\((.+?)\)")

    lines = source_code.split("\n")

    for line in lines:
        match = assignment_pattern.match(line)
        if match:
            var_name, expression = match.groups()
            assignment_map[var_name].append(expression.strip())

            # Track dependencies
            var_refs = re.findall(r"(\b\w+\b)", expression)  # Find variable-like references
            dependencies[var_name].extend(var for var in var_refs if var in assignment_map)

            # Check for merge operations
            merge_match = merge_pattern.search(expression)
            if merge_match:
                left_df, right_df = merge_match.groups()
                dependencies[var_name].extend([left_df, right_df])

            # Detect SQL queries
            sql_match = sql_pattern.search(expression)
            if sql_match:
                assignment_map[var_name].append(f"SQL Query: {sql_match.group(1)}")

            # Detect DataFrame file reads
            df_read_match = dataframe_read_pattern.search(expression)
            if df_read_match:
                read_type, source = df_read_match.groups()
                assignment_map[var_name].append(f"File Read ({read_type}): {source}")

    return assignment_map, dependencies

def trace_variable_path(assignment_map, dependencies, target_var):
    """
    Traces the full path of how a target variable was created, step by step.
    
    Returns:
    - List of tracing steps from the original source(s) to the final variable.
    """
    trace_steps = []
    seen_vars = set()
    stack = [(target_var, 0)]  # Stack stores (variable, indentation level)

    while stack:
        current_var, level = stack.pop()
        if current_var in seen_vars:
            continue  # Prevent infinite loops
        seen_vars.add(current_var)

        if current_var in assignment_map:
            last_assignment = assignment_map[current_var][-1]
            trace_steps.append("  " * level + f"{current_var} = {last_assignment}")

            # If SQL or file read, it's a source
            if "SQL Query:" in last_assignment or "File Read" in last_assignment:
                continue  # Stop tracing this branch further

            # Add dependencies (e.g., in case of merges or chained transformations)
            if current_var in dependencies:
                for dep in dependencies[current_var]:
                    stack.append((dep, level + 1))

    return trace_steps[::-1]  # Reverse order to show origin first

# Example Usage
notebook_path = "your_notebook.ipynb"  # Replace with the actual notebook path
variable_to_trace = "df_2"  # Replace with your target variable

source_code = extract_code_from_notebook(notebook_path)
assignment_map, dependencies = parse_assignments(source_code)
trace_steps = trace_variable_path(assignment_map, dependencies, variable_to_trace)

print("\nTracing Path for Variable:", variable_to_trace)
for step in trace_steps:
    print(step)
