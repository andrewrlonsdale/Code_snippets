import pandas as pd
from pyathena import connect

# Establishing a connection to Athena
conn = connect(s3_staging_dir="s3://your_s3_staging_directory/",
               region_name='your_region_name')

# Fetch column names and data types
query = """
SELECT column_name, data_type 
FROM information_schema.columns 
WHERE table_schema = 'your_database_name' 
AND table_name = 'your_table_name';
"""

column_data = pd.read_sql(query, conn)

# Placeholder for column categorization
column_types = {}

# Check each column
for index, row in column_data.iterrows():
    column_name = row['column_name']
    data_type = row['data_type']

    # If it's numeric, check unique value count
    if data_type in ['int', 'float', 'double']:  # Adjust this list based on your data types in Athena
        unique_count_query = f"""
        SELECT COUNT(DISTINCT {column_name}) 
        FROM your_database_name.your_table_name;
        """
        
        unique_count = pd.read_sql(unique_count_query, conn).iloc[0, 0]
        
        if unique_count <= 10:
            column_types[column_name] = 'categorical'
        else:
            column_types[column_name] = 'continuous'
    
    # If it's string or other non-numeric type
    else:
        column_types[column_name] = 'categorical'

# Convert to DataFrame for better visualization
df_column_types = pd.DataFrame(list(column_types.items()), columns=['Column', 'Type'])

df_column_types






















import boto3
import time

# Initialize Athena client
athena = boto3.client('athena', region_name='your_region_name')

# Specify the S3 location for query results
output_location = 's3://your_s3_query_results_directory/'

# Start the query execution to fetch column names and data types
response = athena.start_query_execution(
    QueryString="""
    SELECT column_name, data_type 
    FROM information_schema.columns 
    WHERE table_schema = 'your_database_name' 
    AND table_name = 'your_table_name';
    """,
    QueryExecutionContext={
        'Database': 'your_database_name'
    },
    ResultConfiguration={
        'OutputLocation': output_location,
    }
)

# Get the query execution ID
query_execution_id = response['QueryExecutionId']

# Poll Athena until the query completes
while True:
    response = athena.get_query_execution(QueryExecutionId=query_execution_id)
    state = response['QueryExecution']['Status']['State']
    
    if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
        break
    
    time.sleep(5)  # Wait for 5 seconds before checking again

# If query succeeded, fetch the results
if state == 'SUCCEEDED':
    results = athena.get_query_results(QueryExecutionId=query_execution_id)
    column_data = results['ResultSet']['Rows'][1:]  # Skipping the header row
    column_data = [{'column_name': row['Data'][0]['VarCharValue'], 'data_type': row['Data'][1]['VarCharValue']} for row in column_data]
    column_data_df = pd.DataFrame(column_data)
else:
    print(f"Query failed with state: {state}")
    column_data_df = None

column_data_df










# Placeholder for column details
column_details = []

# Check each column
for index, row in column_data.iterrows():
    column_name = row['column_name']
    data_type = row['data_type']

    # Details for the current column
    column_info = {'Column Name': column_name}

    # If it's numeric, check unique value count
    if data_type in ['int', 'float', 'double']:  # Adjust this list based on your data types in Athena
        unique_count_query = f"""
        SELECT COUNT(DISTINCT {column_name}) 
        FROM your_database_name.your_table_name;
        """
        
        unique_count = pd.read_sql(unique_count_query, conn).iloc[0, 0]
        
        if unique_count <= 10:
            column_info['Type'] = 'categorical'
            
            # Fetch the top 10 values
            top_values_query = f"""
            SELECT {column_name}, COUNT(*) as count 
            FROM your_database_name.your_table_name 
            GROUP BY {column_name} 
            ORDER BY count DESC 
            LIMIT 10;
            """
            
            top_values = pd.read_sql(top_values_query, conn)
            column_info['Top 10 Values'] = ', '.join(map(str, top_values[column_name].tolist()))
            column_info['Min Value'] = None
            column_info['Max Value'] = None
        else:
            column_info['Type'] = 'continuous'
            
            # Fetch min and max values
            min_max_query = f"""
            SELECT MIN({column_name}) as min_value, MAX({column_name}) as max_value
            FROM your_database_name.your_table_name;
            """
            
            min_max_values = pd.read_sql(min_max_query, conn).iloc[0]
            column_info['Min Value'] = min_max_values['min_value']
            column_info['Max Value'] = min_max_values['max_value']
            column_info['Top 10 Values'] = None
    
    # If it's string or other non-numeric type
    else:
        column_info['Type'] = 'categorical'
        
        # Fetch the top 10 values
        top_values_query = f"""
        SELECT {column_name}, COUNT(*) as count 
        FROM your_database_name.your_table_name 
        GROUP BY {column_name} 
        ORDER BY count DESC 
        LIMIT 10;
        """
        
        top_values = pd.read_sql(top_values_query, conn)
        column_info['Top 10 Values'] = ', '.join(map(str, top_values[column_name].tolist()))
        column_info['Min Value'] = None
        column_info['Max Value'] = None
        
    column_details.append(column_info)

# Convert to DataFrame for better visualization
df_column_details = pd.DataFrame(column_details)

df_column_details



















