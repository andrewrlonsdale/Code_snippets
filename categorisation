import pandas as pd
from pyathena import connect

# Establishing a connection to Athena
conn = connect(s3_staging_dir="s3://your_s3_staging_directory/",
               region_name='your_region_name')

# Fetch column names and data types
query = """
SELECT column_name, data_type 
FROM information_schema.columns 
WHERE table_schema = 'your_database_name' 
AND table_name = 'your_table_name';
"""

column_data = pd.read_sql(query, conn)

# Placeholder for column categorization
column_types = {}

# Check each column
for index, row in column_data.iterrows():
    column_name = row['column_name']
    data_type = row['data_type']

    # If it's numeric, check unique value count
    if data_type in ['int', 'float', 'double']:  # Adjust this list based on your data types in Athena
        unique_count_query = f"""
        SELECT COUNT(DISTINCT {column_name}) 
        FROM your_database_name.your_table_name;
        """
        
        unique_count = pd.read_sql(unique_count_query, conn).iloc[0, 0]
        
        if unique_count <= 10:
            column_types[column_name] = 'categorical'
        else:
            column_types[column_name] = 'continuous'
    
    # If it's string or other non-numeric type
    else:
        column_types[column_name] = 'categorical'

# Convert to DataFrame for better visualization
df_column_types = pd.DataFrame(list(column_types.items()), columns=['Column', 'Type'])

df_column_types






















import boto3
import time

# Initialize Athena client
athena = boto3.client('athena', region_name='your_region_name')

# Specify the S3 location for query results
output_location = 's3://your_s3_query_results_directory/'

# Start the query execution to fetch column names and data types
response = athena.start_query_execution(
    QueryString="""
    SELECT column_name, data_type 
    FROM information_schema.columns 
    WHERE table_schema = 'your_database_name' 
    AND table_name = 'your_table_name';
    """,
    QueryExecutionContext={
        'Database': 'your_database_name'
    },
    ResultConfiguration={
        'OutputLocation': output_location,
    }
)

# Get the query execution ID
query_execution_id = response['QueryExecutionId']

# Poll Athena until the query completes
while True:
    response = athena.get_query_execution(QueryExecutionId=query_execution_id)
    state = response['QueryExecution']['Status']['State']
    
    if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
        break
    
    time.sleep(5)  # Wait for 5 seconds before checking again

# If query succeeded, fetch the results
if state == 'SUCCEEDED':
    results = athena.get_query_results(QueryExecutionId=query_execution_id)
    column_data = results['ResultSet']['Rows'][1:]  # Skipping the header row
    column_data = [{'column_name': row['Data'][0]['VarCharValue'], 'data_type': row['Data'][1]['VarCharValue']} for row in column_data]
    column_data_df = pd.DataFrame(column_data)
else:
    print(f"Query failed with state: {state}")
    column_data_df = None

column_data_df

