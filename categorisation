import pandas as pd
from pyathena import connect

# Establishing a connection to Athena
conn = connect(s3_staging_dir="s3://your_s3_staging_directory/",
               region_name='your_region_name')

# Fetch column names and data types
query = """
SELECT column_name, data_type 
FROM information_schema.columns 
WHERE table_schema = 'your_database_name' 
AND table_name = 'your_table_name';
"""

column_data = pd.read_sql(query, conn)

# Placeholder for column categorization
column_types = {}

# Check each column
for index, row in column_data.iterrows():
    column_name = row['column_name']
    data_type = row['data_type']

    # If it's numeric, check unique value count
    if data_type in ['int', 'float', 'double']:  # Adjust this list based on your data types in Athena
        unique_count_query = f"""
        SELECT COUNT(DISTINCT {column_name}) 
        FROM your_database_name.your_table_name;
        """
        
        unique_count = pd.read_sql(unique_count_query, conn).iloc[0, 0]
        
        if unique_count <= 10:
            column_types[column_name] = 'categorical'
        else:
            column_types[column_name] = 'continuous'
    
    # If it's string or other non-numeric type
    else:
        column_types[column_name] = 'categorical'

# Convert to DataFrame for better visualization
df_column_types = pd.DataFrame(list(column_types.items()), columns=['Column', 'Type'])

df_column_types






















import boto3
import time

# Initialize Athena client
athena = boto3.client('athena', region_name='your_region_name')

# Specify the S3 location for query results
output_location = 's3://your_s3_query_results_directory/'

# Start the query execution to fetch column names and data types
response = athena.start_query_execution(
    QueryString="""
    SELECT column_name, data_type 
    FROM information_schema.columns 
    WHERE table_schema = 'your_database_name' 
    AND table_name = 'your_table_name';
    """,
    QueryExecutionContext={
        'Database': 'your_database_name'
    },
    ResultConfiguration={
        'OutputLocation': output_location,
    }
)

# Get the query execution ID
query_execution_id = response['QueryExecutionId']

# Poll Athena until the query completes
while True:
    response = athena.get_query_execution(QueryExecutionId=query_execution_id)
    state = response['QueryExecution']['Status']['State']
    
    if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
        break
    
    time.sleep(5)  # Wait for 5 seconds before checking again

# If query succeeded, fetch the results
if state == 'SUCCEEDED':
    results = athena.get_query_results(QueryExecutionId=query_execution_id)
    column_data = results['ResultSet']['Rows'][1:]  # Skipping the header row
    column_data = [{'column_name': row['Data'][0]['VarCharValue'], 'data_type': row['Data'][1]['VarCharValue']} for row in column_data]
    column_data_df = pd.DataFrame(column_data)
else:
    print(f"Query failed with state: {state}")
    column_data_df = None

column_data_df










# Placeholder for column details
column_details = []

# Check each column
for index, row in column_data.iterrows():
    column_name = row['column_name']
    data_type = row['data_type']

    # Details for the current column
    column_info = {'Column Name': column_name}

    # If it's numeric, check unique value count
    if data_type in ['int', 'float', 'double']:  # Adjust this list based on your data types in Athena
        unique_count_query = f"""
        SELECT COUNT(DISTINCT {column_name}) 
        FROM your_database_name.your_table_name;
        """
        
        unique_count = pd.read_sql(unique_count_query, conn).iloc[0, 0]
        
        if unique_count <= 10:
            column_info['Type'] = 'categorical'
            
            # Fetch the top 10 values
            top_values_query = f"""
            SELECT {column_name}, COUNT(*) as count 
            FROM your_database_name.your_table_name 
            GROUP BY {column_name} 
            ORDER BY count DESC 
            LIMIT 10;
            """
            
            top_values = pd.read_sql(top_values_query, conn)
            column_info['Top 10 Values'] = ', '.join(map(str, top_values[column_name].tolist()))
            column_info['Min Value'] = None
            column_info['Max Value'] = None
        else:
            column_info['Type'] = 'continuous'
            
            # Fetch min and max values
            min_max_query = f"""
            SELECT MIN({column_name}) as min_value, MAX({column_name}) as max_value
            FROM your_database_name.your_table_name;
            """
            
            min_max_values = pd.read_sql(min_max_query, conn).iloc[0]
            column_info['Min Value'] = min_max_values['min_value']
            column_info['Max Value'] = min_max_values['max_value']
            column_info['Top 10 Values'] = None
    
    # If it's string or other non-numeric type
    else:
        column_info['Type'] = 'categorical'
        
        # Fetch the top 10 values
        top_values_query = f"""
        SELECT {column_name}, COUNT(*) as count 
        FROM your_database_name.your_table_name 
        GROUP BY {column_name} 
        ORDER BY count DESC 
        LIMIT 10;
        """
        
        top_values = pd.read_sql(top_values_query, conn)
        column_info['Top 10 Values'] = ', '.join(map(str, top_values[column_name].tolist()))
        column_info['Min Value'] = None
        column_info['Max Value'] = None
        
    column_details.append(column_info)

# Convert to DataFrame for better visualization
df_column_details = pd.DataFrame(column_details)

df_column_details




import pandas as pd
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor

# Assuming cursor is already initialized and connected to Athena

# Step 1: Fetch unique counts for all columns in one go
count_queries = [f"COUNT(DISTINCT {col}) AS {col}" for col in column_data['column_name']]
count_query = f"""
SELECT
    {', '.join(count_queries)}
FROM
    your_database_name.your_table_name;
"""

unique_counts = cursor.execute(count_query).as_pandas().iloc[0]

# Lists to store columns based on their determined type
continuous_columns = []
categorical_columns = []
high_cardinality_columns = []

# Step 2: Determine column types
for index, row in tqdm(column_data.iterrows(), total=column_data.shape[0], desc="Classifying columns"):
    column_name = row['column_name']
    data_type = row['data_type']
    
    unique_count = unique_counts[column_name]
    
    if data_type in ['int', 'float', 'double'] or 'date' in data_type:
        if 1 < unique_count < 5:
            categorical_columns.append(column_name)
        elif unique_count > 500:
            high_cardinality_columns.append(column_name)
        else:
            continuous_columns.append(column_name)
    elif 'date' not in data_type and unique_count <= 500:
        categorical_columns.append(column_name)
    else:
        high_cardinality_columns.append(column_name)

# Function to fetch min and max for a given column
def fetch_min_max(col):
    query = f"""
    SELECT MIN({col}) AS min_value, MAX({col}) AS max_value
    FROM your_database_name.your_table_name;
    """
    result = cursor.execute(query).as_pandas().iloc[0]
    return col, result['min_value'], result['max_value']

# Function to fetch top 10 values for a given column
def fetch_top_10(col):
    query = f"""
    SELECT {col} AS value, COUNT(*) AS count 
    FROM your_database_name.your_table_name 
    WHERE {col} IS NOT NULL
    GROUP BY {col} 
    ORDER BY count DESC 
    LIMIT 10;
    """
    result = cursor.execute(query).as_pandas()
    return col, result

# Step 3: Parallel fetch min, max for continuous columns and top 10 for categorical columns
column_details = []

with ThreadPoolExecutor() as executor:
    # Fetch min and max values for continuous columns
    for col, min_val, max_val in tqdm(executor.map(fetch_min_max, continuous_columns), total=len(continuous_columns), desc="Fetching min/max values"):
        column_details.append({
            'Column Name': col,
            'SQL Data Type': data_type,
            'Unique Count': unique_counts[col],
            'Type': 'continuous',
            'Min Value': min_val,
            'Max Value': max_val,
            'Top 10 Values': '[Not applicable]'
        })
    
    # Fetch top 10 values for categorical columns
    for col, top_10_df in tqdm(executor.map(fetch_top_10, categorical_columns), total=len(categorical_columns), desc="Fetching top 10 values"):
        top_10_str = ', '.join(map(str, top_10_df['value'].tolist()))
        column_details.append({
            'Column Name': col,
            'SQL Data Type': data_type,
            'Unique Count': unique_counts[col],
            'Type': 'categorical',
            'Min Value': '[Not applicable]',
            'Max Value': '[Not applicable]',
            'Top 10 Values': top_10_str
        })

# Convert to DataFrame for better visualization
df_column_details = pd.DataFrame(column_details)

df_column_details









from tqdm.notebook import tqdm  # Use this if you're in a Jupyter environment
from concurrent.futures import as_completed

# ... [rest of the code]

with ThreadPoolExecutor() as executor:
    # Fetch min and max values for continuous columns
    futures = [executor.submit(fetch_min_max, col) for col in continuous_columns]
    for future in tqdm(as_completed(futures), total=len(continuous_columns), desc="Fetching min/max values"):
        col, min_val, max_val = future.result()
        column_details.append({
            'Column Name': col,
            'SQL Data Type': data_type,
            'Unique Count': unique_counts[col],
            'Type': 'continuous',
            'Min Value': min_val,
            'Max Value': max_val,
            'Top 10 Values': '[Not applicable]'
        })
    
    # Fetch top 10 values for categorical columns
    futures = [executor.submit(fetch_top_10, col) for col in categorical_columns]
    for future in tqdm(as_completed(futures), total=len(categorical_columns), desc="Fetching top 10 values"):
        col, top_10_df = future.result()
        top_10_str = ', '.join(map(str, top_10_df['value'].tolist()))
        column_details.append({
            'Column Name': col,
            'SQL Data Type': data_type,
            'Unique Count': unique_counts[col],
            'Type': 'categorical',
            'Min Value': '[Not applicable]',
            'Max Value': '[Not applicable]',
            'Top 10 Values': top_10_str
        })
































import pandas as pd
from pyathena import connect
import concurrent.futures

# Establish Athena connection
conn = connect(s3_staging_dir='s3://your-path/',
               region_name='your-region')

# Define threshold for categorical vs continuous determination
threshold = 0.05 * 2_000_000  # 5% of total rows

def process_chunk(chunk):
    result = []
    for column in chunk.columns:
        unique_values = chunk[column].nunique()
        if unique_values <= threshold:
            # Categorical Field
            top_10_values = chunk[column].value_counts().head(10)
            result.append((column, 'Categorical', top_10_values))
        else:
            # Continuous Field
            min_val = chunk[column].min()
            max_val = chunk[column].max()
            result.append((column, 'Continuous', (min_val, max_val)))
    return result

# Process data in chunks
chunksize = 10000  # Adjust this value based on your system's memory
results = []
for chunk in pd.read_sql('SELECT * FROM your_table', conn, chunksize=chunksize):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future = executor.submit(process_chunk, chunk)
        results.extend(future.result())

# Convert results to a DataFrame
final_result = pd.DataFrame(results, columns=['Field', 'Type', 'Values'])









def determine_field_type(chunk):
    field_types = {}
    for column in chunk.columns:
        if chunk[column].dtype in ['int64', 'float64']:
            field_types[column] = 'continuous'
        else:
            field_types[column] = 'categorical'
    return field_types

def compute_statistics(chunk, field_types):
    statistics = {}
    for column, field_type in field_types.items():
        if field_type == 'continuous':
            statistics[column] = {
                'min': chunk[column].min(),
                'max': chunk[column].max()
            }
        else:
            top_categories = chunk[column].value_counts().head(10).index.tolist()
            statistics[column] = {
                'top_categories': top_categories
            }
    return statistics

# Assume cursor is already set up
query = "SELECT * FROM your_table"
cursor.execute(query)

chunksize = 10000  # Adjust chunk size as per your system's capacity
while True:
    rows = cursor.fetchmany(chunksize)
    if not rows:
        break
    chunk = pd.DataFrame(rows, columns=[desc[0] for desc in cursor.description])  # Assuming column names are in the description
    field_types = determine_field_type(chunk)
    statistics = compute_statistics(chunk, field_types)
    # Process statistics here or store them for later processing
    ...

# Remember to close the cursor and connection when done
cursor.close()

























import pandas as pd
import multiprocessing

def determine_field_type(chunk):
    field_types = {}
    for column in chunk.columns:
        if chunk[column].dtype in ['int64', 'float64']:
            field_types[column] = 'continuous'
        else:
            field_types[column] = 'categorical'
    return field_types

def compute_statistics(chunk, field_types):
    statistics = {}
    for column, field_type in field_types.items():
        if field_type == 'continuous':
            statistics[column] = {
                'min': chunk[column].min(),
                'max': chunk[column].max()
            }
        else:
            top_categories = chunk[column].value_counts().head(10).index.tolist()
            statistics[column] = {
                'top_categories': top_categories
            }
    return statistics

def process_chunk(rows):
    chunk = pd.DataFrame(rows, columns=[desc[0] for desc in cursor.description])
    field_types = determine_field_type(chunk)
    statistics = compute_statistics(chunk, field_types)
    return statistics

# Assume cursor is already set up
query = "SELECT * FROM your_table"
cursor.execute(query)

chunksize = 10000  # Adjust chunk size as per your system's capacity

# Initialize storage for global statistics
global_min_max = {}
global_category_counts = {}

# Setup a Pool for parallel processing
pool = multiprocessing.Pool()

while True:
    rows = cursor.fetchmany(chunksize)
    if not rows:
        break

    # Process chunks in parallel
    statistics_list = pool.map(process_chunk, [rows])

    # Update global statistics
    for statistics in statistics_list:
        for column, stats in statistics.items():
            if field_types[column] == 'continuous':
                global_min_max.setdefault(column, {'min': float('inf'), 'max': float('-inf')})
                global_min_max[column]['min'] = min(global_min_max[column]['min'], stats['min'])
                global_min_max[column]['max'] = max(global_min_max[column]['max'], stats['max'])
            else:
                global_category_counts.setdefault(column, {})
                for category in stats['top_categories']:
                    global_category_counts[column][category] = global_category_counts[column].get(category, 0) + 1

# Close the Pool
pool.close()
pool.join()

# Remember to close the cursor and connection when done
cursor.close()

# Convert global statistics to DataFrame and write to CSV
pd.DataFrame(global_min_max).T.to_csv('continuous_statistics.csv')
pd.DataFrame({column: pd.Series(counts) for column, counts in global_category_counts.items()}).to_csv('categorical_statistics.csv')







