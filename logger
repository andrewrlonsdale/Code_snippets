import os
import boto3
import pandas as pd
import logging
from datetime import datetime

# Log file configuration
LOG_FILE = 'file_processing.log'
LOG_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'

# Create a logger and configure log file
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
file_handler = logging.FileHandler(LOG_FILE)
file_handler.setLevel(logging.INFO)
formatter = logging.Formatter(LOG_FORMAT)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# Get the list of files in the S3 input folder
response = s3_client.list_objects_v2(Bucket=BUCKET_NAME, Prefix=S3_INPUT_FOLDER)

# Process each file
for file in response['Contents']:
    file_key = file['Key']
    file_name = os.path.basename(file_key)
    
    # Move the file to the completed folder
    new_key = f'{S3_COMPLETED_FOLDER}/{file_name}'
    s3_client.copy_object(Bucket=BUCKET_NAME, CopySource=f'{BUCKET_NAME}/{file_key}', Key=new_key)
    s3_client.delete_object(Bucket=BUCKET_NAME, Key=file_key)
    
    # Read the file data into a DataFrame (modify this according to your file format)
    file_object = s3_client.get_object(Bucket=BUCKET_NAME, Key=new_key)
    df = pd.read_csv(file_object['Body'])
    
        # Log the file processing
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_message = f"Processed file '{file_name}' at {timestamp}"
    logger.info(log_message)
    print(log_message)

print("All files processed.")
